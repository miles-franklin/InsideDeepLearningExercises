{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4abc72e",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f23d6e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "# Standard Imports\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# Ch 2 Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import *\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Ch 3 Imports\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "# Now you can import your file just like any other library\n",
    "import sys\n",
    "sys.path.append('../Inside-Deep-Learning/')\n",
    "\n",
    "from idlmam import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69c43855",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xpu\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "   device = torch.device(\"cuda\")\n",
    "   B = 32\n",
    "   epochs = 20\n",
    "   num_workers_data_loaders = 2\n",
    "elif torch.xpu.is_available():\n",
    "   device = torch.device(\"xpu\")\n",
    "   B = 32\n",
    "   epochs = 20\n",
    "   num_workers_data_loaders = 2\n",
    "else:\n",
    "   device = torch.device(\"cpu\")\n",
    "   B = 1\n",
    "   epochs = 5\n",
    "   num_workers_data_loaders = 1\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bd2f0f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import BytesIO \n",
    "from zipfile import ZipFile \n",
    "from urllib.request import urlopen \n",
    "import re\n",
    "\n",
    "all_data = []\n",
    "resp = urlopen(\n",
    " \"https://cs.stanford.edu/people/karpathy/char-rnn/shakespear.txt\") \n",
    "shakespear_100k = resp.read() \n",
    "shakespear_100k = shakespear_100k.decode('utf-8').lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a2975779",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Size:  36\n",
      "Total Characters: 99993\n"
     ]
    }
   ],
   "source": [
    "vocab2indx = {}\n",
    "for char in shakespear_100k: \n",
    "    if char not in vocab2indx:\n",
    "        vocab2indx[char] = len(vocab2indx)\n",
    "\n",
    "\n",
    "indx2vocab = {}\n",
    "for k, v in vocab2indx.items():\n",
    "    indx2vocab[v] = k \n",
    "    \n",
    "print(\"Vocab Size: \", len(vocab2indx)) \n",
    "print(\"Total Characters:\", len(shakespear_100k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a7cb7e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoRegressiveDataset(Dataset): \n",
    "    \"\"\" \n",
    "    Creates an autoregressive dataset from one single, long, source \n",
    "     sequence by breaking it up into \"chunks\". \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, large_string, max_chunk=500): \n",
    "        \"\"\" \n",
    "        large_string: the original long source sequence that chunks will \n",
    "         be extracted from \n",
    "        max_chunk: the maximum allowed size of any chunk. \n",
    "        \"\"\"\n",
    "        self.doc = large_string \n",
    "        self.max_chunk = max_chunk\n",
    "\n",
    "    def __len__(self): \n",
    "        return (len(self.doc)-1) // self.max_chunk\n",
    "\n",
    "    def __getitem__(self, idx): \n",
    "        start = idx*self.max_chunk\n",
    "\n",
    "        sub_string = self.doc[start:start+self.max_chunk]\n",
    "        x = [vocab2indx[c] for c in sub_string]\n",
    "        sub_string = self.doc[start+1:start+self.max_chunk+1]\n",
    "        y = [vocab2indx[c] for c in sub_string]\n",
    "        return torch.tensor(x, dtype=torch.int64), torch.tensor(y, \n",
    "         dtype=torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e9b4a0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoRegressive(nn.Module):\n",
    "\n",
    "    def __init__(self, num_embeddings, embd_size, hidden_size, layers=1):\n",
    "       super(AutoRegressive, self).__init__() \n",
    "\n",
    "       self.hidden_size = hidden_size \n",
    "       self.embd = nn.Embedding(num_embeddings, embd_size)\n",
    "\n",
    "       self.layers = nn.ModuleList(\n",
    "            [nn.GRUCell(embd_size, hidden_size)] + [nn.GRUCell(hidden_size, hidden_size) \n",
    "            for i in range(layers-1)])\n",
    "       self.norms = nn.ModuleList( \n",
    "            [nn.LayerNorm(hidden_size) for i in range(layers)])\n",
    "       \n",
    "       self.pred_class = nn.Sequential( \n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.LeakyReLU(), \n",
    "            nn.LayerNorm(hidden_size),\n",
    "            nn.Linear(hidden_size, num_embeddings)\n",
    "        )\n",
    "    \n",
    "    def forward(self, input):\n",
    "        B = input.size(0)\n",
    "        T = input.size(1)\n",
    "\n",
    "        x = self.embd(input)\n",
    "\n",
    "        h_prevs = self.initHiddenStates(B)\n",
    "\n",
    "        last_activations = [] \n",
    "        for t in range(T):\n",
    "            x_in = x[:,t,:]\n",
    "            last_activations.append(self.step(x_in, h_prevs))\n",
    "\n",
    "        last_activations = torch.stack(last_activations, dim=1)\n",
    "\n",
    "        return last_activations\n",
    "    \n",
    "    def initHiddenStates(self, B): \n",
    "        \"\"\" \n",
    "        Creates an initial hidden state list for the RNN layers.\n",
    "\n",
    "        B: the batch size for the hidden states. \n",
    "        \"\"\" \n",
    "        return [torch.zeros(B, self.hidden_size, device=device) \n",
    "            for _ in range(len(self.layers))]\n",
    "    \n",
    "    def step(self, x_in, h_prevs=None): \n",
    "        \"\"\" \n",
    "        x_in: the input for this current time step and has shape (B) \n",
    "        if the values need to be embedded, and (B, D) if they \n",
    "        have already been embedded.\n",
    "\n",
    "        h_prevs: a list of hidden state tensors each with shape \n",
    "        (B, self.hidden_size) for each layer in the network.\n",
    "        These contain the current hidden state of the RNN layers \n",
    "        and will be updated by this call. \n",
    "        \"\"\"\n",
    "\n",
    "        if len(x_in.shape) == 1:\n",
    "            x_in = self.embd(x_in)\n",
    "\n",
    "        if h_prevs is None:\n",
    "            h_prevs = self.initHiddenStates(x_in.shape[0])\n",
    "\n",
    "        for l in range(len(self.layers)):\n",
    "            h_prev = h_prevs[l] \n",
    "            h = self.norms[l](self.layers[l](x_in, h_prev))\n",
    "\n",
    "            h_prevs[l] = h \n",
    "            x_in = h \n",
    "        return self.pred_class(x_in)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "49f18995",
   "metadata": {},
   "outputs": [],
   "source": [
    "def applyLinearLayerOverTime(x): \n",
    "    results = []\n",
    "    B, T, D = x.shape \n",
    "    for t in range(T): \n",
    "        results.append(linearLayer(x[:,t,:]))\n",
    "        \n",
    "    return torch.stack(results, dim=0).view(B, T, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ffc8b7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "autoRegData = AutoRegressiveDataset(shakespear_100k, max_chunk=250) \n",
    "autoReg_loader = DataLoader(autoRegData, batch_size=128, shuffle=True)\n",
    "\n",
    "autoReg_model = AutoRegressive(len(vocab2indx), 32, 128, layers=2) \n",
    "autoReg_model = autoReg_model.to(device)\n",
    "\n",
    "for p in autoReg_model.parameters(): \n",
    "   p.register_hook(lambda grad: torch.clamp(grad, -2, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6bc39b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CrossEntLossTime(x, y): \n",
    "    \"\"\" \n",
    "    x: output with shape (B, T, V) \n",
    "    y: labels with shape (B, T) \n",
    "    \"\"\" \n",
    "    cel = nn.CrossEntropyLoss() \n",
    "\n",
    "    T = x.size(1) \n",
    "\n",
    "    loss = 0 \n",
    "\n",
    "    for t in range(T):\n",
    "        loss += cel(x[:,t,:], y[:,t])\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a9c1ff40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|██████████| 100/100 [12:29<00:00,  7.49s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>total time</th>\n",
       "      <th>train loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>8.266254</td>\n",
       "      <td>884.772064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>15.722022</td>\n",
       "      <td>803.757401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>22.679129</td>\n",
       "      <td>772.206879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>30.018151</td>\n",
       "      <td>765.550522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>38.546090</td>\n",
       "      <td>761.632721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>95</td>\n",
       "      <td>706.699120</td>\n",
       "      <td>411.351776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>96</td>\n",
       "      <td>713.628780</td>\n",
       "      <td>410.847778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>97</td>\n",
       "      <td>722.492518</td>\n",
       "      <td>405.594604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>98</td>\n",
       "      <td>735.530222</td>\n",
       "      <td>408.191254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>99</td>\n",
       "      <td>748.599694</td>\n",
       "      <td>410.741486</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    epoch  total time  train loss\n",
       "0       0    8.266254  884.772064\n",
       "1       1   15.722022  803.757401\n",
       "2       2   22.679129  772.206879\n",
       "3       3   30.018151  765.550522\n",
       "4       4   38.546090  761.632721\n",
       "..    ...         ...         ...\n",
       "95     95  706.699120  411.351776\n",
       "96     96  713.628780  410.847778\n",
       "97     97  722.492518  405.594604\n",
       "98     98  735.530222  408.191254\n",
       "99     99  748.599694  410.741486\n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_network(autoReg_model, CrossEntLossTime, autoReg_loader, epochs=100, \n",
    " device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326359ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87aae10e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
