{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "829a2448",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b66e0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "# Standard Imports\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# Ch 2 Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import *\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Ch 3 Imports\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "# Now you can import your file just like any other library\n",
    "import sys\n",
    "sys.path.append('../Inside-Deep-Learning/')\n",
    "\n",
    "from idlmam import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f048a486",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xpu\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "   device = torch.device(\"cuda\")\n",
    "   B = 32\n",
    "   epochs = 20\n",
    "   num_workers_data_loaders = 2\n",
    "elif torch.xpu.is_available():\n",
    "   device = torch.device(\"xpu\")\n",
    "   B = 32\n",
    "   epochs = 20\n",
    "   num_workers_data_loaders = 2\n",
    "else:\n",
    "   device = torch.device(\"cpu\")\n",
    "   B = 1\n",
    "   epochs = 5\n",
    "   num_workers_data_loaders = 1\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbdaff40",
   "metadata": {},
   "source": [
    "# 1\\. Now that you know how to enlarge a tensor after pooling, you can implement a convolutional autoencoder using only the bottleneck approach. Go back to chapter 7 and reimplement a convolutional autoencoder by using two rounds of pooling in the encoder countered by two rounds of transposed convolutions in the decoder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdbde6f9",
   "metadata": {},
   "source": [
    "## Get MNIST data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0fad32d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = torchvision.datasets.MNIST(\"./\", train=True,\n",
    " transform=transforms.ToTensor(), download=True)\n",
    "test_data = torchvision.datasets.MNIST(\"./\", train=True,\n",
    " transform=transforms.ToTensor(), download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d0a012b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncodeDataset(Dataset): \n",
    "    \"\"\"Takes a dataset with (x, y) label pairs and converts it to (x, x) pairs. \n",
    "    This makes it easy to reuse other code\"\"\"\n",
    "\n",
    "    def __init__(self, dataset): \n",
    "        self.dataset = dataset\n",
    "\n",
    "    def __len__(self): \n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx): \n",
    "        x, y = self.dataset.__getitem__(idx) \n",
    "        return x, x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "251ac9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_xy = train_data\n",
    "train_data_xx = AutoEncodeDataset(train_data)\n",
    "\n",
    "test_data_xy = test_data\n",
    "test_data_xx = AutoEncodeDataset(test_data)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_data_xx, batch_size=256, shuffle=True)\n",
    "test_loader = DataLoader(test_data_xx, batch_size=256)\n",
    "\n",
    "D = 28*28\n",
    "C = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b6643c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "D = 28*28\n",
    "n = 2\n",
    "C = 1\n",
    "classes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2f853cca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 2)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data), len(train_data[0]), "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44422f99",
   "metadata": {},
   "source": [
    "## Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ff9f44e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ConvLayer(c_in, c_out=None, filter_size=3):\n",
    "   if c_out is None:\n",
    "        c_out = c_in\n",
    "\n",
    "   layer = nn.Sequential(\n",
    "      nn.Conv2d(c_in, c_out, filter_size, padding=filter_size//2),\n",
    "      nn.Tanh(),\n",
    "   )\n",
    "\n",
    "   return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49dfc761",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_filters = 32\n",
    "\n",
    "segmentation_model2 = nn.Sequential( \n",
    "    ConvLayer(C, n_filters),\n",
    "    ConvLayer(n_filters, n_filters), \n",
    "\n",
    "    nn.MaxPool2d(2),\n",
    "    ConvLayer(n_filters, 2*n_filters), \n",
    "    ConvLayer(2*n_filters, 2*n_filters),\n",
    "\n",
    "    nn.MaxPool2d(2),\n",
    "    ConvLayer(2*n_filters, 4*n_filters), \n",
    "    ConvLayer(4*n_filters, 4*n_filters), \n",
    "\n",
    "    nn.ConvTranspose2d(4*n_filters, 2*n_filters, (3,3),\n",
    "                       padding=1, output_padding=1, stride=2), \n",
    "    nn.BatchNorm2d(2*n_filters), \n",
    "    nn.LeakyReLU(),\n",
    "    ConvLayer(2*n_filters, 2*n_filters),\n",
    "    \n",
    "    nn.ConvTranspose2d(2*n_filters, n_filters, (3,3),\n",
    "                       padding=1, output_padding=1, stride=2), \n",
    "    nn.BatchNorm2d(n_filters), \n",
    "    nn.LeakyReLU(), \n",
    "\n",
    "    ConvLayer(n_filters, n_filters),\n",
    "    nn.Conv2d(n_filters, 1, (3,3), padding=1),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "39cbdebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_filters = 32\n",
    "\n",
    "auto_encoder = nn.Sequential(\n",
    "    ConvLayer(C, n_filters),\n",
    "    ConvLayer(n_filters, n_filters), \n",
    "\n",
    "    nn.MaxPool2d(2),\n",
    "    ConvLayer(n_filters, 2*n_filters), \n",
    "    ConvLayer(2*n_filters, 2*n_filters),\n",
    "\n",
    "    nn.MaxPool2d(2),\n",
    "    ConvLayer(2*n_filters, 4*n_filters), \n",
    "    ConvLayer(4*n_filters, 4*n_filters)\n",
    ")\n",
    "\n",
    "# The decoder is a convolutional network\n",
    "auto_decoder = nn.Sequential(\n",
    "    nn.ConvTranspose2d(4*n_filters, 2*n_filters, (3,3),\n",
    "                       padding=1, output_padding=1, stride=2), \n",
    "    nn.BatchNorm2d(2*n_filters), \n",
    "    nn.LeakyReLU(),\n",
    "    ConvLayer(2*n_filters, 2*n_filters),\n",
    "    \n",
    "    nn.ConvTranspose2d(2*n_filters, n_filters, (3,3),\n",
    "                       padding=1, output_padding=1, stride=2), \n",
    "    nn.BatchNorm2d(n_filters), \n",
    "    nn.LeakyReLU(), \n",
    "\n",
    "    nn.Conv2d(n_filters, classes, (3,3), padding=1)\n",
    ")\n",
    "\n",
    "model = nn.Sequential(\n",
    "    auto_encoder, \n",
    "    auto_decoder \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4776e3e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   0%|          | 0/4 [00:00<?, ?it/s]C:\\Users\\miles\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\nn\\modules\\loss.py:616: UserWarning: Using a target size (torch.Size([256, 1, 28, 28])) that is different to the input size (torch.Size([256, 10, 28, 28])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "Epoch:   0%|          | 0/4 [02:53<?, ?it/s]\n",
      "c:\\Users\\miles\\Dev\\InsideDeepLearningExercises\\../Inside-Deep-Learning\\idlmam.py:215: SyntaxWarning: invalid escape sequence '\\e'\n",
      "  \"\"\"Train simple neural networks\n"
     ]
    }
   ],
   "source": [
    "loss = nn.MSELoss()\n",
    "\n",
    "results_model_cnn = train_network(model,\n",
    "   loss, train_loader, test_loader=test_loader,\n",
    "   score_funcs={'Accuracy': accuracy_score}, device=device, epochs=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6139f08",
   "metadata": {},
   "source": [
    "# 2\\. You may have noticed that a transposed convolution can create unevenly spaced artifacts in its output, which occur in our example diagram. These are not always a problem, but you can do better. Implement your own Conv2dExpansion(n_filters_in) class that takes the following approach: first, up-sample the image using nn.Upsample to expand the tensor width and height by a factor of 2. If you are off by a pixel, use nn.ReflectionPad2d to pad the output to the desired shape. Finally, apply a normal nn.Conv2d to perform some mixing and change the number of channels. Compare this new approach with transposed convolution and see if you can identify any pros and cons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73cce7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
